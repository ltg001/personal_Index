# Machine Learning 相关

摘录来源：西瓜书

## 基本术语

分类任务：预测的是离散值

    二分类：正类 + 反类
    多分类：涉及多个类别
回归任务：预测的是连续值

聚类：自动形成的簇对应一些潜在的概念划分 了解数据潜在的规律 `这样的概念我们事先是不知道的 在学习中的训练样本一般没有标记信息`

监督学习：回归任务
非监督学习：聚类任务

泛化能力：学得模型适用于新样本的能力 有强泛化能力的模型会很好的适用于整个样本空间 （样本是样本空间的很小的采样 但是希望他能够很好的适用于整个样本空间）一般来说 训练样本越多 泛化能力越强

假设空间规模：在每个类别的可能取值数量上 `+1` 因为考虑这个类别不存在的情况

版本空间：一个有多个假设和训练集一致的假设空间

归纳偏好：在学习过程中对某种假设类型的偏好

“奥卡姆剃刀”原则：若有多个假设和观察一致 选择最简单那个

## 模型选择和评估

错误率：分类错误的样本数占样本总数的比例 error rate
精度：分类正确的样本数占样本总数的比例 accuracy

查准率：precision 准确率
查全率：recall 召回率

训练误差：在训练集上的误差 training error / empirical error
泛化误差：在新样本上的误差 generalization error

方差：同样大小训练集上变动导致的学习性能的的变化 ===> 数据扰动带来的影响
偏差：预测和真实结果的偏离程度 ===> 算法本身的拟合能力
噪声：任何学习算法能达到的期望泛化误差的下界

过拟合：学习能力过强（由算法和数据共同决定） 学到了训练样本中没有一般性的特征 不好克服

    特征：方差 > 偏差 ===> 学到了不具有泛化特性的特征 加正则化项
欠拟合：学习能力低下 ===> 决策树扩展分支 增加训练轮数

    特征：偏差 > 方差 ===> 拟合能力不强 可以加 epoch 数据
规范化（归一化）：将不同范围变化的值映射到固定的范围 $[0, 1]$
### 评估方法

明确：*测试集应与训练集互斥*

1. 留出法
将数据集划分为互斥的两个集合 一个训练一个测试
注意在两个集合中保证数据分布的一致性 ===> 分层采样 （一般使用 $2/3 - 4/5$作为训练集）

        若为分类任务 保证样本类别比例相似

如果类别比例差异比较大 误差估计会因为 训练/测试数据分布的差异产生偏差
缺点：估计结果不够可靠 ===> 一般进行多次随机划分取平均值作为评估结果

2. 交叉验证法
先将数据集 $D$ 划分为 $k$ 个大小相似的互斥子集 每个子集保证数据分布的一致性
将 $k-1$ 个作为训练集 余下的作为测试集 ===> 进行 k 次训练 将平均值记为评估结果
因为训练结果很大程度上依赖 $k$ 的选择 称为 `k折交叉验证`

因为也和集合的划分有关 一般重复 $p$ 次 ===> 称为`p次k折交叉验证`

3. 自助采样法 bootstrapping
对数据集 $D$ 进行随机抽样 经过 $m$ 次始终没被抽到的概率约为 $1/e = 0.368$
将所有抽样的集合记为 $D'$ 将其作为训练集 将 $D-D'$ 作为测试集
在数据集较小时可以使用
缺点：改变了初始数据集的数据分布 会引入估计偏差

4. 调参和模型
训练用 -> 训练集
模型评估测试 -> 验证集
泛化能力 -> 训练集

### 性能度量

对比不同的模型好坏时 使用不同的性能度量会导致评判结果不同 ===> 模型的好坏是相对的

**回归任务常用**：均方误差 mean squared error

**混淆矩阵**：
|真实\\预测|正例|反例|
|:----:|:---:|:----:|
|正例|$TP$|$FN$|
|反例|$FP$|$TN$|

$$recall = \frac{TP}{TP+FN}$$
$$precision = \frac{TP}{TP+FP}$$

>理解：
>**precision**：在所有的判断为真的样本中真实情况为真的占的比例
>**recall**：在所有真实情况为真的样本中被判断出来为真的比例
>===> 两个指标大部分情况下不可兼得

#### P-R曲线

两个坐标轴分别为 precision 和 recall 曲线一定经过 $(1, 0)$ 和 $(0, 1)$
模型优劣评定：看曲线包括的区域面积
平衡点（BEP break-event point）：此时 准确率 == 召回率 时的取值 可以通过 BEP 的大小衡量模型优劣

**F1度量**：基于 准确率 和 召回率 的调和平均定义
> 相比算术平均和几何平均 调和平均更加重视较小值

$$F1 = \frac{2 \times TP}{样例总数 + TP - TN}$$

类似定义 $F_\beta$ 表达对两个指标的不同偏好

$$F_\beta = \frac{(1 + \beta)^{2}\times Precision \times Recall}{(\beta^{2}\times Precision) + Recall}$$
显然 $\beta = 1$时退化为 $F1$
当 $\beta > 1$时注重 召回率 Recall
当 $\beta < 1$时注重 准确率 Precision

对*多个*混淆矩阵求总的准确率和召回率：

1. 宏 macro

> macro-$Precision = \frac{1}{n} \sum_{i=1}^{n} Precision_i$ <br>
> macro-$Recall = \frac{1}{n} \sum_{i=1}^{n}Recall_i$ <br>
> 算每个矩阵的准确率和召回率的平均

2. 微 micro
> micro-$Precision = \frac{\overline{TP}}{\overline{TP} + \overline{FP}}$ <br>
> micro-$Recall = \frac{\overline{TP}}{\overline{TP} + \overline{FN}}$ <br>
> micro-$F1 = \frac{2 \times micro-P \times micro-R}{micro-P + micro-R}$ <br>
> 先算每个矩阵对应元素的平均 再算准确率和召回率

#### ROC曲线

受试者工作特征 Receiver Operating Characteristic

**TPR** 真正例率 在所有真实情况为真的样本中判断为真的占的比例
$$TPR = \frac{TP}{TP + FN}$$
**FPR** 假正例率 在所有真实情况为假的样本中判断为真的占的比例
$$FPR = \frac{FP}{TP + FP}$$

**ROC图** 两个坐标轴是 TPR和FPR 一定过$(0, 0)$和$(1, 1)$

AUC：ROC曲线下的面积 模型的AUC较大的效果较好

$$AUC = \frac{1}{2}\sum_{i = 1}^{m - 1}(x_{i+1} - x_i)(y_i+y_{i+1})$$

形式化的来看 AUC 考虑样本的排序质量 和排序误差有紧密联系
考虑每一对正反例 若正例的预测值小于反例 则计一个“罚分” 若相等 计0.5个“罚分”
显然 $l_{rank}$ 对应的是ROC曲线上的面积：若一个正例在ROC曲线上对应标记点的坐标为$(x, y)$ 则 x 恰是排序在其之前的反例所占的比例 即假正例率 有 $AUC = 1 - loss_{rank}$

**代价矩阵**：
|真实\\预测|第0类|第1类|
|:----:|:----:|:---:|
|第0类|$0$|$cost_{01}$|
|第1类|$cost_{10}$|$0$|

$cost_{ij}$表示将第i个类别预测为第j个类别的代价 一般情况下重要的不是 cost 的绝对值而是 cost 的比值

若有 ROC 曲线上的点坐标为 $(FPR, TPR)$ 可以计算出 $FNR$ 在代价平面上绘制一条从 (0, FPR) 到 (1, FNR) 的线段 线段下的面积表示该条件下的期望总体代价 将每个曲线上的点转化成代价平面上的一条线段 取所有线段的下界 即在所有条件下的期望总体代价